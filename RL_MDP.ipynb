{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2204ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32bc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "goal_reward = 10\n",
    "dead_end_reward = -10\n",
    "obstacle_reward = np.nan #cannot enter state\n",
    "step_reward = -0.05\n",
    "\n",
    "up = 0\n",
    "down = 1\n",
    "left = 2\n",
    "right = 3\n",
    "actions = [up, down, left, right]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a11f0",
   "metadata": {},
   "source": [
    "def create_grid_world():\n",
    "    grid = np.full((grid_size, grid_size), step_reward)\n",
    "    grid[grid_size-1, grid_size-1] = goal_reward\n",
    "    grid[grid_size-2, grid_size-1] = dead_end_reward\n",
    "    \n",
    "    #obstacles\n",
    "    num_obstacles = 10\n",
    "    obstacle_coords = []\n",
    "    while len(obstacle_coords) < num_obstacles:\n",
    "        row = random.randint(0, grid_size-1)\n",
    "        col = random.randint(0, grid_size-1)\n",
    "        if (row, col) not in (obstacle_coords) and (row, col) != (grid_size-1, grid_size-1) and (row, col) != (grid_size-2, grid_size-1):\n",
    "            obstacle_coords.append((row, col))\n",
    "            grid[row, col] = obstacle_reward\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fec4e2",
   "metadata": {},
   "source": [
    "def is_valid_state(state):\n",
    "    row, col = state\n",
    "    if row < grid_size and col < grid_size:\n",
    "        return 0<=row<grid_size and 0<=col<grid_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38462b",
   "metadata": {},
   "source": [
    "def next_state(state, action):\n",
    "    row, col = state\n",
    "    if action == up:\n",
    "        next_row, next_col = row-1, col\n",
    "    elif action == down:\n",
    "        next_row, next_col = row+1, col\n",
    "    elif action == right:\n",
    "        next_row, next_col = row, col+1\n",
    "    elif action == left:\n",
    "        next_row, next_col = row, col-1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Action\")\n",
    "        \n",
    "    if is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33c9da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (partial view):\n",
      "(0, 0): Obstacle/Not in Policy  (0, 1): DOWN  (0, 2): DOWN  (0, 3): DOWN  (0, 4): DOWN  \n",
      "(1, 0): DOWN  (1, 1): DOWN  (1, 2): DOWN  (1, 3): DOWN  (1, 4): DOWN  \n",
      "(2, 0): DOWN  (2, 1): DOWN  (2, 2): DOWN  (2, 3): DOWN  (2, 4): DOWN  \n",
      "(3, 0): DOWN  (3, 1): DOWN  (3, 2): DOWN  (3, 3): DOWN  (3, 4): DOWN  \n",
      "(4, 0): DOWN  (4, 1): DOWN  (4, 2): DOWN  (4, 3): DOWN  (4, 4): DOWN  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Grid world dimensions\n",
    "GRID_SIZE = 10\n",
    "\n",
    "# Define rewards\n",
    "GOAL_REWARD = 10\n",
    "DEAD_END_REWARD = -10\n",
    "OBSTACLE_REWARD = np.nan  # Represents a state you can't enter\n",
    "STEP_REWARD = -0.05\n",
    "\n",
    "# Actions\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "\n",
    "\n",
    "def create_grid_world():\n",
    "    \"\"\"Creates the grid world with rewards and obstacles.\"\"\"\n",
    "    grid = np.full((GRID_SIZE, GRID_SIZE), STEP_REWARD)\n",
    "    grid[GRID_SIZE - 1, GRID_SIZE - 1] = GOAL_REWARD\n",
    "    grid[GRID_SIZE - 2, GRID_SIZE - 1] = DEAD_END_REWARD\n",
    "\n",
    "    # Place random obstacles\n",
    "    num_obstacles = 10\n",
    "    obstacle_coords = []\n",
    "    while len(obstacle_coords) < num_obstacles:\n",
    "        row = random.randint(0, GRID_SIZE - 1)\n",
    "        col = random.randint(0, GRID_SIZE - 1)\n",
    "        if (row, col) not in obstacle_coords and (row, col) != (GRID_SIZE - 1, GRID_SIZE - 1) and (row, col) != (GRID_SIZE - 2, GRID_SIZE - 1) :\n",
    "            obstacle_coords.append((row, col))\n",
    "            grid[row, col] = OBSTACLE_REWARD\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def is_valid_state(state):\n",
    "    \"\"\"Checks if a state is within the grid bounds.\"\"\"\n",
    "    row, col = state\n",
    "    return 0 <= row < GRID_SIZE and 0 <= col < GRID_SIZE\n",
    "\n",
    "\n",
    "def next_state(state, action):\n",
    "    \"\"\"Calculates the next state based on the current state and action.\"\"\"\n",
    "    row, col = state\n",
    "    if action == UP:\n",
    "        next_row, next_col = row - 1, col\n",
    "    elif action == DOWN:\n",
    "        next_row, next_col = row + 1, col\n",
    "    elif action == LEFT:\n",
    "        next_row, next_col = row, col - 1\n",
    "    elif action == RIGHT:\n",
    "        next_row, next_col = row, col + 1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action.\")\n",
    "\n",
    "    if is_valid_state((next_row, next_col)):\n",
    "        return next_row, next_col\n",
    "    else:\n",
    "        return row, col  # Stay in the same state if the next state is invalid\n",
    "\n",
    "\n",
    "def define_policy():\n",
    "\n",
    "    #Policy 1: Always move right then down, until goal or dead-end is reached or obstacle is encountered\n",
    "    policy1 = {}\n",
    "    for i in range(0,GRID_SIZE):\n",
    "        for j in range(0,GRID_SIZE):\n",
    "            policy1[(i,j)] = RIGHT if j < GRID_SIZE -1 else DOWN\n",
    "\n",
    "    #Policy 2: Move randomly\n",
    "    policy2 = {}\n",
    "    for i in range(0, GRID_SIZE):\n",
    "        for j in range(0, GRID_SIZE):\n",
    "            policy2[(i, j)] = random.choice(ACTIONS)\n",
    "\n",
    "\n",
    "    #Policy 3: Always move down if you are in first half of the columns then always right\n",
    "    policy3 = {}\n",
    "    for i in range(0,GRID_SIZE):\n",
    "        for j in range(0,GRID_SIZE):\n",
    "            policy3[(i,j)] = DOWN if j < GRID_SIZE//2 else RIGHT\n",
    "\n",
    "    return policy1, policy2, policy3\n",
    "\n",
    "\n",
    "def value_iteration(grid, policy, discount_factor=0.99, theta=0.001):\n",
    "    \"\"\"Performs value iteration to find the optimal value function.\"\"\"\n",
    "\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for row in range(GRID_SIZE):\n",
    "            for col in range(GRID_SIZE):\n",
    "                if not np.isnan(grid[row, col]):  # Skip obstacles\n",
    "                    v = V[row, col]\n",
    "                    state = (row, col)\n",
    "\n",
    "                    action = policy[state]\n",
    "                    next_s = next_state(state, action)\n",
    "\n",
    "                    reward = grid[next_s[0],next_s[1]]\n",
    "                    \n",
    "                    #If obstacle is encountered\n",
    "                    if np.isnan(reward):\n",
    "                        reward = grid[state[0], state[1]] #Stay in same state with corresponding reward\n",
    "                        next_s = state\n",
    "\n",
    "                    V[row, col] = reward + discount_factor * V[next_s[0],next_s[1]]\n",
    "\n",
    "\n",
    "                    delta = max(delta, abs(v - V[row, col]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def policy_evaluation(grid, policy, discount_factor=0.99, theta=0.001):\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for row in range(GRID_SIZE):  # Iterate over rows\n",
    "            for col in range(GRID_SIZE):  # Iterate over cols\n",
    "                s = (row, col)             # Create the state tuple\n",
    "                if s in policy:  # Check if state is in policy (might not be for obstacles)\n",
    "                    v = V[s]\n",
    "                    a = policy[s]\n",
    "                    next_s = next_state(s, a)\n",
    "                    reward = grid[next_s] if not np.isnan(grid[next_s]) else grid[s]\n",
    "                    next_s = next_s if not np.isnan(grid[next_s]) else s\n",
    "                    V[s] = reward + discount_factor * V[next_s]\n",
    "                    delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "\n",
    "def policy_improvement(grid, V, discount_factor=0.99):\n",
    "    policy = {}\n",
    "    for row in range(GRID_SIZE):         # Iterate by row\n",
    "        for col in range(GRID_SIZE):      # Iterate by column\n",
    "            s = (row, col)              # Create the state tuple\n",
    "\n",
    "            if not np.isnan(grid[s]):   # Only update policy for non-obstacle states\n",
    "                best_action = None\n",
    "                best_value = -np.inf\n",
    "                for a in ACTIONS:\n",
    "                    next_s = next_state(s, a)\n",
    "                    reward = grid[next_s] if not np.isnan(grid[next_s]) else grid[s]\n",
    "                    next_s = next_s if not np.isnan(grid[next_s]) else s\n",
    "                    value = reward + discount_factor * V[next_s]\n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_action = a\n",
    "                policy[s] = best_action  # Store best action for the current state\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(grid, discount_factor=0.99):\n",
    "    policy1, policy2, policy3 = define_policy()\n",
    "    policy = policy1 #Initializing policy\n",
    "\n",
    "    while True:\n",
    "        V = policy_evaluation(grid, policy, discount_factor)\n",
    "        new_policy = policy_improvement(grid, V, discount_factor)\n",
    "\n",
    "\n",
    "        if new_policy == policy:\n",
    "            break\n",
    "\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "grid = create_grid_world()\n",
    "\n",
    "policy1, policy2, policy3 = define_policy()\n",
    "V_policy1 = value_iteration(grid, policy1)\n",
    "V_policy2 = value_iteration(grid, policy2)\n",
    "V_policy3 = value_iteration(grid, policy3)\n",
    "\n",
    "optimal_V, optimal_policy = policy_iteration(grid)\n",
    "\n",
    "\n",
    "print(\"\\nOptimal Policy (partial view):\") # Added a comment\n",
    "for row in range(min(GRID_SIZE, 5)):   # Print only a portion if the grid is large\n",
    "    for col in range(min(GRID_SIZE, 5)):\n",
    "        s = (row, col)\n",
    "        if s in optimal_policy:\n",
    "            action_str = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"][optimal_policy[s]]\n",
    "            print(f\"({row}, {col}): {action_str}  \", end=\"\")\n",
    "        else:\n",
    "            print(f\"({row}, {col}): Obstacle/Not in Policy  \", end=\"\")  # Indicate if obstacle\n",
    "    print()  # Newline after each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a4bbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[[ 1.20164244e+00 -7.08133215e-02 -7.12750914e-02 -7.47543017e-02]\n",
      "  [ 2.46251412e-01 -9.03943933e-02  1.44542745e+00  4.04319488e-01]\n",
      "  [-1.08635988e-01 -3.07563118e-02  1.62795095e+00 -1.64863893e-01]\n",
      "  [ 1.87905968e+00  1.34857476e-01 -1.58579615e-01  1.83670358e-02]\n",
      "  [ 6.43638682e-02  2.42468991e-01  2.16884335e+00  2.77824214e-01]\n",
      "  [-7.44865697e-02  5.86419029e-02  2.45277366e+00  5.12335260e-02]\n",
      "  [ 2.77616078e+00  1.60230408e-01  3.13858505e-01  3.40405303e-01]\n",
      "  [ 7.23362907e-01  5.62682810e-01  3.16099252e+00  1.21633075e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 6.08860168e-02  9.62943149e-02  2.45081510e+00 -1.30149813e-01]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 3.47932516e-01  5.66931150e-01  1.66185617e+00 -5.12245847e-02]\n",
      "  [ 4.32674276e-01 -1.58234496e-01  1.90191983e+00 -1.57550073e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 5.04321434e-01  2.69718186e-01  2.46550983e+00  6.21477365e-01]\n",
      "  [ 5.92109907e-01  1.04456414e+00  2.79495457e+00  6.34208751e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.82918860e+00  2.00125554e+00  3.56791513e+00  2.00329848e+00]\n",
      "  [ 9.46707570e-01  3.16112361e+00  8.50948459e-01  1.06824143e+00]\n",
      "  [ 6.41290056e-01  2.79497095e+00  3.24398020e-01  5.40974333e-01]]\n",
      "\n",
      " [[ 1.64870583e+00 -1.58661735e-01  2.18289129e-02 -1.58716894e-01]\n",
      "  [ 9.91190823e-01  6.03536032e-01  1.90206321e+00  8.71104025e-01]\n",
      "  [ 2.10066625e-01  7.64756534e-01  2.16895910e+00  3.36227195e-01]\n",
      "  [ 3.60406259e-01  4.22116670e-01  2.46315276e+00  6.27015844e-01]\n",
      "  [ 8.33869783e-01  6.21271340e-01  2.79501126e+00  6.11122481e-01]\n",
      "  [ 1.31696425e+00  3.70317330e-01  3.16112356e+00  5.69062838e-01]\n",
      "  [ 8.86546266e-01  1.88806639e-01  3.56734865e+00 -9.77600656e-02]\n",
      "  [ 1.94331987e+00  1.97511822e+00  4.01990570e+00  2.13137846e+00]\n",
      "  [ 4.18064205e-01  3.56783993e+00  7.74745023e-01  8.68692979e-01]\n",
      "  [-9.51105458e-02  3.09403220e+00 -9.68559843e-02  4.22172355e-01]]\n",
      "\n",
      " [[ 1.90051329e+00  1.39395409e-01 -1.50302195e-01 -1.50834655e-01]\n",
      "  [ 2.16895912e+00  7.62286403e-01  1.24122182e+00  5.39246353e-01]\n",
      "  [ 2.46551013e+00  1.61708930e+00  1.97276879e+00  1.66263460e+00]\n",
      "  [ 2.79501126e+00  1.64326591e+00  2.34498095e+00  1.32862161e+00]\n",
      "  [ 2.96396174e+00  2.22909487e+00  3.16112362e+00  1.96560406e+00]\n",
      "  [ 3.56791513e+00  1.67304765e+00  1.81414633e+00  1.50920706e+00]\n",
      "  [ 2.97557161e+00  2.00247002e+00  4.01990570e+00  2.34090631e+00]\n",
      "  [ 3.14752911e+00  3.35453585e+00  4.52211745e+00  2.88418230e+00]\n",
      "  [ 1.25197154e+00  4.01990250e+00  1.04750914e+00  1.08760081e+00]\n",
      "  [-8.29645802e-02  3.56321995e+00 -2.05118229e-02  6.75852537e-01]]\n",
      "\n",
      " [[ 2.14071627e+00 -1.48276153e-01  8.46976313e-02  1.57441518e-01]\n",
      "  [ 2.46544406e+00  5.68849387e-02  7.63300146e-01  4.91297671e-01]\n",
      "  [ 2.79501119e+00  1.01190198e+00  7.92949640e-01  4.80767586e-01]\n",
      "  [ 3.16112362e+00  1.23151573e+00  3.73084626e-01  1.26764839e+00]\n",
      "  [ 3.56791513e+00  2.74552994e+00  3.01217215e+00  2.75501801e+00]\n",
      "  [ 4.01990570e+00  3.03632801e+00  3.46259693e+00  3.09904502e+00]\n",
      "  [ 4.52211745e+00  3.53471213e+00  4.50278322e+00  3.56265266e+00]\n",
      "  [ 4.01665859e+00  4.01589094e+00  5.08013050e+00  4.01942852e+00]\n",
      "  [ 1.20043646e+00  4.52211744e+00  1.92838087e+00  1.16167331e+00]\n",
      "  [ 6.20354657e-01  4.01606579e+00  8.82724746e-01  7.88618855e-01]]\n",
      "\n",
      " [[ 2.45268689e+00 -1.43196622e-01  6.60357270e-02  4.85702718e-01]\n",
      "  [ 5.39047058e-01  6.26875026e-01  2.79482869e+00  5.60687995e-01]\n",
      "  [ 4.93871543e-01  5.59371692e-01  3.15881766e+00  1.32396365e-01]\n",
      "  [-1.26377078e-01 -7.94660346e-02  3.40760048e-01  2.78503476e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 2.86863480e+00  2.06613434e+00  5.08013048e+00  1.34768324e+00]\n",
      "  [ 4.51654590e+00  4.51546391e+00  5.70014500e+00  4.51213055e+00]\n",
      "  [ 9.81164136e-01  5.08012541e+00  8.13167878e-01  9.12318457e-01]\n",
      "  [ 3.60848572e-01  3.76975948e-01  5.65953131e+00  3.08634105e-01]]\n",
      "\n",
      " [[ 1.66145825e-01  8.85068550e-02  2.74456538e+00 -9.47065621e-03]\n",
      "  [ 3.16112340e+00 -1.21453301e-01  8.51324994e-01  3.65802898e-01]\n",
      "  [ 1.42231050e+00  1.40468737e+00  3.56791513e+00  1.49141775e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 5.06081569e+00  8.07508160e-01  3.76036201e-01  6.07792179e-01]\n",
      "  [ 5.70014500e+00  2.37031413e+00  4.59664488e+00  2.63785109e+00]\n",
      "  [ 5.69778900e+00  5.07987821e+00  6.38905000e+00  5.07918572e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 9.57003440e-01  5.38137619e-01  6.38899708e+00  6.41662989e-01]]\n",
      "\n",
      " [[ 3.16083055e+00  3.51091892e-01  2.00210641e-01 -2.69008009e-02]\n",
      "  [ 3.56791508e+00  9.50217711e-01  8.74215933e-01  9.81610169e-01]\n",
      "  [ 2.69085965e+00  2.78333949e+00  4.01990570e+00  1.94791409e+00]\n",
      "  [ 5.37993912e-01  9.21752017e-01  4.52197065e+00  7.00589143e-01]\n",
      "  [ 3.85737031e-01  9.27862107e-01  5.05423950e+00 -5.22560962e-02]\n",
      "  [ 1.51754094e+00 -4.30379421e-02  5.64549011e+00  4.00761143e-01]\n",
      "  [ 6.38904965e+00  1.55518200e+00  2.18177997e+00  1.35760864e+00]\n",
      "  [ 7.15294706e+00  5.69622960e+00  7.15450000e+00  5.69969785e+00]\n",
      "  [ 5.26618540e+00  3.63429855e+00  8.00500000e+00  4.65626547e+00]\n",
      "  [ 1.37253031e+00  7.15449999e+00 -4.68559000e+00  1.83861794e+00]]\n",
      "\n",
      " [[-1.14064340e-01  4.60262727e-01  3.54449115e+00  1.62543243e-01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 4.52211745e+00  3.07881510e+00  3.33784693e+00  2.89163985e+00]\n",
      "  [ 3.45903773e+00  3.50391900e+00  5.08013050e+00  3.44519009e+00]\n",
      "  [ 1.87656463e+00  4.16007757e-01  5.70013785e+00  1.67789218e+00]\n",
      "  [ 5.93547179e-01  1.35234142e+00  6.38904440e+00  5.28601604e-01]\n",
      "  [ 1.92541580e+00  5.23468421e-01  7.15449983e+00  1.52744180e+00]\n",
      "  [ 8.00500000e+00  6.38696284e+00  8.00339686e+00  6.38767602e+00]\n",
      "  [-9.99856659e+00  7.15349345e+00  8.95000000e+00  7.15308981e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 4.01979416e+00  8.84422058e-01  1.30879306e+00  9.40355374e-01]\n",
      "  [ 4.52211721e+00  1.27076619e+00  2.04809300e+00  2.58068383e+00]\n",
      "  [ 5.08013050e+00  1.50057573e+00  1.80645398e+00  1.70093000e+00]\n",
      "  [ 5.70014500e+00  4.15415231e+00  4.71232775e+00  4.39887502e+00]\n",
      "  [ 6.38905000e+00  5.00983918e+00  5.29931118e+00  4.73500303e+00]\n",
      "  [ 7.15450000e+00  5.55831873e+00  6.33625331e+00  5.49256788e+00]\n",
      "  [ 8.00500000e+00  6.21857297e+00  7.07065048e+00  6.26343231e+00]\n",
      "  [ 8.95000000e+00  7.10388472e+00  7.91752822e+00  7.06727141e+00]\n",
      "  [ 1.00000000e+01  8.00499100e+00  8.94999610e+00  8.00499521e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "\n",
      "Optimal action for state (0, 0): (0, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=(10, 10)):\n",
    "        self.size = size\n",
    "        self.grid = np.zeros(self.size)\n",
    "        self.goal = (size[0]-1, size[1]-1)\n",
    "        self.dead_end = (size[0]-2, size[1]-1)  # Adjusted dead end\n",
    "        self.obstacles = self._generate_obstacles(10)  # 10 random obstacles\n",
    "\n",
    "        self.grid[self.goal] = 10\n",
    "        self.grid[self.dead_end] = -10\n",
    "\n",
    "        for obs in self.obstacles:\n",
    "            self.grid[obs] = np.nan\n",
    "\n",
    "\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)] # Right, Left, Down, Up\n",
    "\n",
    "    def _generate_obstacles(self, num_obstacles):\n",
    "        obstacles = []\n",
    "        while len(obstacles) < num_obstacles:\n",
    "            obs = (random.randint(0, self.size[0]-1), random.randint(0, self.size[1]-1))\n",
    "            if obs != self.goal and obs != self.dead_end and obs not in obstacles:\n",
    "                obstacles.append(obs)\n",
    "        return obstacles\n",
    "\n",
    "\n",
    "    def step(self, state, action):\n",
    "        new_state = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "        if not (0 <= new_state[0] < self.size[0] and 0 <= new_state[1] < self.size[1]):  # Out of bounds\n",
    "            return state, -0.05  # Small negative reward for hitting a wall\n",
    "        elif new_state in self.obstacles:\n",
    "             return state, -0.05  # Penalty for trying to move into obstacles\n",
    "        elif new_state == self.goal:\n",
    "            return new_state, 10\n",
    "        elif new_state == self.dead_end:\n",
    "            return new_state, -10\n",
    "        else:\n",
    "            return new_state, -0.05\n",
    "\n",
    "    def reset(self):\n",
    "        # Start at a random non-goal, non-obstacle state\n",
    "        while True:\n",
    "          start_state = (random.randint(0, self.size[0]-1), random.randint(0, self.size[1]-1))\n",
    "          if start_state != self.goal and start_state != self.dead_end and start_state not in self.obstacles:\n",
    "             return start_state\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.q_table = np.zeros((env.size[0], env.size[1], len(env.actions)))\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon # Exploration rate\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.env.actions) # Explore\n",
    "        else:\n",
    "            return self.env.actions[np.argmax(self.q_table[state])]  # Exploit\n",
    "\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.max(self.q_table[next_state])\n",
    "        self.q_table[state][self.env.actions.index(action)] = (1 - self.alpha) * self.q_table[state][self.env.actions.index(action)] + \\\n",
    "                                                       self.alpha * (reward + self.gamma * best_next_action)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "grid_env = GridWorld()\n",
    "agent = QLearningAgent(grid_env)\n",
    "\n",
    "num_episodes = 5000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = grid_env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward = grid_env.step(state, action)\n",
    "        agent.update_q_table(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if state == grid_env.goal or state == grid_env.dead_end:\n",
    "            done = True\n",
    "\n",
    "\n",
    "# After training, print the optimal policy:\n",
    "print(\"Learned Q-table:\")\n",
    "print(agent.q_table)  \n",
    "\n",
    "# Example of getting optimal actions for a specific state:\n",
    "state = (0,0)\n",
    "optimal_action_index = np.argmax(agent.q_table[state])\n",
    "optimal_action = grid_env.actions[optimal_action_index]\n",
    "print(f\"\\nOptimal action for state {state}: {optimal_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1dbdb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal action for state (9, 8): (0, 1)\n"
     ]
    }
   ],
   "source": [
    "state = (9, 8)\n",
    "optimal_action_index = np.argmax(agent.q_table[state])\n",
    "optimal_action = grid_env.actions[optimal_action_index]\n",
    "print(f\"\\nOptimal action for state {state}: {optimal_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b0a942",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(env, policies)\n\u001b[0;32m    103\u001b[0m start_state \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Example start state\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m total_reward, path \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgreedy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Reward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_reward)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
      "Cell \u001b[1;32mIn[2], line 81\u001b[0m, in \u001b[0;36mAgent.run_episode\u001b[1;34m(self, policy, start_state)\u001b[0m\n\u001b[0;32m     78\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgrid[current_state[\u001b[38;5;241m0\u001b[39m],current_state[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgrid[current_state[\u001b[38;5;241m0\u001b[39m],current_state[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 81\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnext_state(current_state, action)\n\u001b[0;32m     83\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_state_reward(next_state)\n",
      "Cell \u001b[1;32mIn[2], line 68\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, state, policy)\u001b[0m\n\u001b[0;32m     66\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnext_state(state, action)\n\u001b[0;32m     67\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_state_reward(next_state)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(reward) \u001b[38;5;129;01mand\u001b[39;00m reward \u001b[38;5;241m>\u001b[39m best_q:  \u001b[38;5;66;03m# Handle obstacles\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     best_q \u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     70\u001b[0m     best_action \u001b[38;5;241m=\u001b[39m action\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define the environment\n",
    "class Environment:\n",
    "    def __init__(self, csv_file):\n",
    "        self.grid = pd.read_csv(csv_file, header=None).values\n",
    "        self.rows, self.cols = self.grid.shape\n",
    "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)] # Right, Left, Down, Up\n",
    "\n",
    "\n",
    "    def get_state_reward(self, state):\n",
    "        row, col = state\n",
    "        cell = self.grid[row, col]\n",
    "        if cell == 0:\n",
    "            return -0.05\n",
    "        elif cell == 'X':\n",
    "            return np.nan  # Obstacle\n",
    "        elif cell == 'D':\n",
    "            return -10\n",
    "        elif cell == 'G':\n",
    "            return 10\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def is_valid_state(self, state):\n",
    "         row, col = state\n",
    "         return 0 <= row < self.rows and 0 <= col < self.cols and self.grid[row, col] != 'X'\n",
    "\n",
    "\n",
    "    def next_state(self, current_state, action):\n",
    "        row, col = current_state\n",
    "        dr, dc = action\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        if self.is_valid_state((new_row, new_col)):\n",
    "            return (new_row, new_col)\n",
    "        else:\n",
    "            return current_state  # Stay in the same state if the move is invalid\n",
    "\n",
    "\n",
    "    def update_grid(self, path):\n",
    "        for row, col in path:\n",
    "            if self.grid[row, col] == 0:  # Only replace '0' cells \n",
    "                 self.grid[row, col] = 'A'\n",
    "        self.save_grid(\"updated_grid.csv\")\n",
    "\n",
    "    def save_grid(self, filename):\n",
    "        pd.DataFrame(self.grid).to_csv(filename, header=False, index=False)\n",
    "\n",
    "\n",
    "# Define the agent\n",
    "class Agent:\n",
    "    def __init__(self, env, policies, gamma = 0.9): # Discount Factor\n",
    "        self.env = env\n",
    "        self.policies = policies\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def choose_action(self, state, policy):\n",
    "        if policy == 'random':\n",
    "            return random.choice(self.env.actions)\n",
    "        elif policy == 'greedy':\n",
    "            best_action = None\n",
    "            best_q = -float('inf')\n",
    "            for action in self.env.actions:\n",
    "                next_state = self.env.next_state(state, action)\n",
    "                reward = self.env.get_state_reward(next_state)\n",
    "                if not np.isnan(reward) and reward > best_q:  # Handle obstacles\n",
    "                    best_q = reward\n",
    "                    best_action = action\n",
    "            return best_action if best_action is not None else random.choice(self.env.actions) #If no valid action found\n",
    "        #Add more polices if needed\n",
    "\n",
    "\n",
    "    def run_episode(self, policy, start_state): #added start state\n",
    "        path = []\n",
    "        current_state = start_state\n",
    "        total_reward = 0\n",
    "\n",
    "        while self.env.grid[current_state[0],current_state[1]] != 'G' and self.env.grid[current_state[0],current_state[1]] != 'D':\n",
    "            action = self.choose_action(current_state, policy)\n",
    "            next_state = self.env.next_state(current_state, action)\n",
    "            reward = self.env.get_state_reward(next_state)\n",
    "            \n",
    "            if not np.isnan(reward): # Skip Obstacles\n",
    "                total_reward += reward\n",
    "                path.append(current_state) # adding previous state as we have already moved to next state\n",
    "                current_state = next_state\n",
    "\n",
    "        # Check if episode finished or reached an obstacle        \n",
    "        if self.env.grid[current_state[0],current_state[1]] == 'G' or self.env.grid[current_state[0],current_state[1]] == 'D':\n",
    "            path.append(current_state)\n",
    "\n",
    "\n",
    "        return total_reward, path\n",
    "\n",
    "\n",
    "# Example usage\n",
    "env = Environment(\"grid.csv\")\n",
    "policies = ['random', 'greedy']\n",
    "agent = Agent(env, policies)\n",
    "\n",
    "start_state = (0,0) # Example start state\n",
    "total_reward, path = agent.run_episode('greedy', start_state)\n",
    "\n",
    "print(\"Total Reward:\", total_reward)\n",
    "print(\"Path:\", path)\n",
    "\n",
    "env.update_grid(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ec296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (speed)",
   "language": "python",
   "name": "speed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
